import os
import torch
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# --- T·∫Øt Triton v√† TorchInductor ---
torch._dynamo.config.suppress_errors = True
torch._dynamo.config.disable = True

# (Tu·ª≥ ch·ªçn) B·∫≠t h·ªó tr·ª£ TF32 n·∫øu card GPU c√≥ Tensor Core
torch.set_float32_matmul_precision('high')

# --- Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env ---
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

# --- C√°c h√†m helper ---
def _load_model_and_tokenizer():
    """
    Load m√¥ h√¨nh g·ªëc v√† m√¥ h√¨nh ƒë√£ fine-tune (ch·ªâ ch·∫°y m·ªôt l·∫ßn).
    Tr·∫£ v·ªÅ tokenizer v√† model ƒë√£ s·∫µn s√†ng ƒë·ªÉ sinh vƒÉn b·∫£n.
    """
    base_model = "google/gemma-3-1b-it"
    model_dir = "./fine_tuned_model"  # ƒê∆∞·ªùng d·∫´n ch·ª©a tr·ªçng s·ªë LoRA

    tokenizer = AutoTokenizer.from_pretrained(base_model, token=HF_TOKEN)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        device_map="auto",
        token=HF_TOKEN
    )
    model = PeftModel.from_pretrained(model, model_dir)
    model.eval()

    return tokenizer, model

def _generate_response(prompt, tokenizer, model):
    """
    Sinh c√¢u tr·∫£ l·ªùi d·ª±a tr√™n prompt ng∆∞·ªùi d√πng.
    Tr·∫£ v·ªÅ chu·ªói vƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω.
    """
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=2048
    ).to("cuda")

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.8,
            top_k=50,
            top_p=0.95
        )

    full_output = tokenizer.decode(output[0], skip_special_tokens=True)

    # X·ª≠ l√Ω h·∫≠u k·ª≥: lo·∫°i b·ªè prompt n·∫øu b·ªã l·∫∑p l·∫°i
    if full_output.startswith(prompt):
        return full_output[len(prompt):].strip()
    return full_output.strip()

# --- H√†m ch√≠nh input ‚Üí output ---
def get_health_advice(user_input):
    """
    Nh·∫≠n input ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi t·ª´ model.
    V√≠ d·ª•: "T·∫°i sao r·ª≠a tay quan tr·ªçng?" ‚Üí "R·ª≠a tay gi√∫p ph√≤ng ng·ª´a vi khu·∫©n..."
    """
    # Lazy load m√¥ h√¨nh
    if not hasattr(get_health_advice, 'tokenizer'):
        get_health_advice.tokenizer, get_health_advice.model = _load_model_and_tokenizer()

    # Prompt m·∫´u
    prompt = (
        "### Vai tr√≤:\n"
        "B·∫°n l√† tr·ª£ l√Ω ·∫£o chuy√™n v·ªÅ s·ª©c kh·ªèe c·ªông ƒë·ªìng t·∫°i Vi·ªát Nam.\n\n"
        "### Y√™u c·∫ßu:\n"
        "- Tr·∫£ l·ªùi r√µ r√†ng, ch√≠nh x√°c, d·ªÖ hi·ªÉu, ng·∫Øn g·ªçn.\n"
        "- N·∫øu kh√¥ng bi·∫øt th√¨ ph·∫£i tr·∫£ l·ªùi l√† kh√¥ng bi·∫øt.\n\n"
        f"### C√¢u h·ªèi:\n{user_input}\n\n"
        "### Tr·∫£ l·ªùi:"
    )

    # Sinh c√¢u tr·∫£ l·ªùi
    return _generate_response(prompt, get_health_advice.tokenizer, get_health_advice.model)

# --- Ch·∫°y th·ª≠ t·ª´ command line ---
if __name__ == "__main__":
    print("ü©∫ Tr·ª£ l√Ω s·ª©c kh·ªèe AI. G√µ c√¢u h·ªèi v√† nh·∫•n Enter (ƒë·ªÉ tho√°t, nh·∫•n Enter r·ªóng).")
    while True:
        user_question = input("üßë B·∫°n h·ªèi: ").strip()
        if not user_question:
            print("T·∫°m bi·ªát! üëã")
            break
        answer = get_health_advice(user_question)
        print("ü§ñ Tr·∫£ l·ªùi:", answer)
